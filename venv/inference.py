import os
import cv2
import torch
import numpy as np
from ultralytics import YOLO
from model import LSTMPoseClassifier  # ëª¨ë¸ ì •ì˜ê°€ ë“¤ì–´ìˆëŠ” íŒŒì¼ë¡œë¶€í„° import
import argparse

CLASS_NAMES = [
    "Fall", "Damage", "Fire", "Smoke", "Abandon", "Theft", "Assault"
]


# ğŸ“Œ ì¶”ë¡  í•¨ìˆ˜
def run_inference(video_path, model_path, sequence_length=45):
    # 1. ëª¨ë¸ ë¡œë“œ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    lstm_model = LSTMPoseClassifier(input_size=34, hidden_size=128, num_layers=2, num_classes=7)
    lstm_model.load_state_dict(torch.load(model_path, map_location=device))
    lstm_model.to(device).eval()

    # 2. YOLO ë¡œë“œ
    pose_model = YOLO("yolov8n-pose.pt")

    # 3. ë¹„ë””ì˜¤ ì—´ê¸°
    cap = cv2.VideoCapture(video_path)
    keypoints_list = []
    frames = []
    results = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)

        # YOLO ì¶”ë¡ 
        result = pose_model(frame)[0]
        keypoints = result.keypoints
        if keypoints is not None and len(keypoints.xy) > 0:
            xy = keypoints.xy[0].cpu().numpy().flatten()
            keypoints_list.append(xy if len(xy) == 34 else np.zeros(34))
        else:
            keypoints_list.append(np.zeros(34))

    cap.release()
    keypoints_array = np.array(keypoints_list)

    # 4. ì‹œí€€ìŠ¤ ë‹¨ìœ„ë¡œ ì¶”ë¡ 
    for i in range(0, len(keypoints_array) - sequence_length + 1):
        seq = keypoints_array[i:i + sequence_length]
        seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            pred = lstm_model(seq_tensor)
            pred_class = pred.argmax(dim=1).item()
        results.append(pred_class)

    # 5. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    from collections import Counter
    counter = Counter(results)
    print("\nğŸ¯ ì¶”ë¡  ê²°ê³¼ (ì‹œí€€ìŠ¤ ë‹¨ìœ„)")
    for class_idx, count in counter.most_common():
        print(f"  - {CLASS_NAMES[class_idx]}: {count}íšŒ")

    # 6. í”„ë ˆì„ë³„ë¡œ í´ë˜ìŠ¤ ì´ë¦„ ì¶œë ¥ (ì„ íƒ)
    print("\nğŸ¬ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì‹œí€€ìŠ¤:")
    print([CLASS_NAMES[r] for r in results])

# ğŸ“Œ ì‹¤í–‰
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--video", type=str, required=True, help="ì˜ˆì¸¡í•  ì˜ìƒ ê²½ë¡œ (.mp4)")
    parser.add_argument("--model", type=str, required=True, help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (.pth)")
    args = parser.parse_args()

    run_inference(args.video, args.model)
